import { VertexAI } from '@google-cloud/vertexai';
import { VertexAIConfig, VertexAIGenerationConfig, VertexAIError } from '../types/vertex-ai';
import { SearchResult, ConversationMessage } from '../types';
import { LLMProvider } from './llm-service';

export class VertexAILLMService implements LLMProvider {
  private vertexAI: VertexAI;
  private model: string;
  private config: VertexAIConfig;
  private generationConfig: VertexAIGenerationConfig;

  constructor(config: VertexAIConfig) {
    this.config = config;
    this.model = config.textModel || 'gemini-1.5-pro';
    this.generationConfig = {
      temperature: 0.1,
      topP: 0.95,
      topK: 32,
      maxOutputTokens: 2048,
      ...config.generationConfig,
    };
    
    console.log(`üîó Initializing Vertex AI LLM Service`);
    console.log(`üìç Project: ${config.projectId}, Location: ${config.location}`);
    console.log(`ü§ñ Model: ${this.model}`);

    // Initialize Vertex AI client
    const authOptions: any = {
      projectId: config.projectId,
    };

    if (config.keyFilename) {
      authOptions.keyFilename = config.keyFilename;
      console.log(`üîë Using service account key: ${config.keyFilename}`);
    } else {
      console.log(`üîë Using Application Default Credentials (ADC)`);
    }

    this.vertexAI = new VertexAI({
      project: config.projectId,
      location: config.location,
      googleAuthOptions: authOptions,
    });

    console.log(`‚úÖ Vertex AI LLM Service initialized`);
  }

  async generateResponse(
    prompt: string, 
    context: SearchResult[], 
    conversation?: ConversationMessage[]
  ): Promise<string> {
    try {
      const model = this.vertexAI.getGenerativeModel({
        model: this.model,
        generationConfig: this.generationConfig,
      });

      const systemPrompt = this.buildSystemPrompt();
      const contextPrompt = this.buildContextPrompt(context);
      const conversationHistory = this.buildConversationHistory(conversation || []);
      
      const fullPrompt = `${systemPrompt}

${contextPrompt}

${conversationHistory}

„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè: ${prompt}

ÂõûÁ≠î:`;

      const result = await model.generateContent(fullPrompt);
      
      if (!result.response) {
        throw new VertexAIError('No response received from Vertex AI');
      }

      const responseText = result.response.candidates?.[0]?.content?.parts?.[0]?.text || 'No response text available';
      
      if (!responseText) {
        throw new VertexAIError('Empty response from Vertex AI');
      }

      return responseText;
    } catch (error) {
      console.error('Failed to generate response with Vertex AI:', error);
      
      if (error instanceof Error) {
        throw new VertexAIError(
          `Vertex AI response generation failed: ${error.message}`,
          'GENERATION_ERROR',
          undefined,
          error
        );
      }
      
      throw new VertexAIError('Unknown error in Vertex AI response generation');
    }
  }

  async generateRelatedQuestions(query: string, context: SearchResult[]): Promise<string[]> {
    try {
      const model = this.vertexAI.getGenerativeModel({
        model: this.model,
        generationConfig: {
          ...this.generationConfig,
          temperature: 0.3, // Slightly higher temperature for creativity
          maxOutputTokens: 1024,
        },
      });

      const contextSummary = context
        .slice(0, 3) // Use top 3 results
        .map(result => result.chunk.content.substring(0, 200))
        .join('\n\n');

      const prompt = `‰ª•‰∏ã„ÅÆ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Å®„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè„Å´Âü∫„Å•„ÅÑ„Å¶„ÄÅÈñ¢ÈÄ£„Åô„ÇãË≥™Âïè„Çí3„Å§ÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà:
${contextSummary}

„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè: ${query}

Èñ¢ÈÄ£„Åô„ÇãË≥™Âïè„Çí3„Å§„ÄÅ‰ª•‰∏ã„ÅÆÂΩ¢Âºè„ÅßÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ:
1. [Ë≥™Âïè1]
2. [Ë≥™Âïè2] 
3. [Ë≥™Âïè3]

ÂõûÁ≠î:`;

      const result = await model.generateContent(prompt);
      
      if (!result.response) {
        return [];
      }

      const responseText = result.response.candidates?.[0]?.content?.parts?.[0]?.text || '';
      
      // Parse the numbered list format
      const questions = responseText
        .split('\n')
        .map((line: string) => line.trim())
        .filter((line: string) => /^\d+\./.test(line))
        .map((line: string) => line.replace(/^\d+\.\s*/, ''))
        .filter((question: string) => question.length > 0)
        .slice(0, 3); // Ensure we only return 3 questions

      return questions;
    } catch (error) {
      console.error('Failed to generate related questions with Vertex AI:', error);
      return []; // Return empty array on error
    }
  }

  private buildSystemPrompt(): string {
    return `„ÅÇ„Å™„Åü„ÅØÊó•Êú¨„ÅÆÊ≥ïÂæãÊñáÊõ∏„Å´ÁâπÂåñ„Åó„ÅüÂ∞ÇÈñÄÁöÑ„Å™„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆ„Ç¨„Ç§„Éâ„É©„Ç§„É≥„Å´Âæì„Å£„Å¶ÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ:

1. Êèê‰æõ„Åï„Çå„Åü„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÅÆÊÉÖÂ†±„ÅÆ„Åø„Çí‰ΩøÁî®„Åó„Å¶ÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ
2. Ê≠£Á¢∫„ÅßÂÖ∑‰ΩìÁöÑ„Å™ÊÉÖÂ†±„ÇíÊèê‰æõ„Åó„ÄÅÊõñÊòß„Å™Ë°®Áèæ„ÅØÈÅø„Åë„Å¶„Åè„Å†„Åï„ÅÑ
3. Ê≥ïÂæãÁî®Ë™û„ÅØÈÅ©Âàá„Å´‰ΩøÁî®„Åó„ÄÅÂøÖË¶Å„Å´Âøú„Åò„Å¶Ë™¨Êòé„ÇíÂä†„Åà„Å¶„Åè„Å†„Åï„ÅÑ
4. ÂõûÁ≠î„ÅÆÊ†πÊã†„Å®„Å™„ÇãÊñáÊõ∏„ÇÑÊù°Êñá„ÇíÊòéÁ§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ
5. ÊÉÖÂ†±„Åå‰∏çÂçÅÂàÜ„Å™Â†¥Âêà„ÅØ„ÄÅ„Åù„ÅÆÊó®„ÇíÊòéÁ¢∫„Å´Ëø∞„Åπ„Å¶„Åè„Å†„Åï„ÅÑ
6. Êó•Êú¨Ë™û„Åß‰∏ÅÂØß„Åã„Å§Â∞ÇÈñÄÁöÑ„Å™Ë™ûË™ø„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ
7. Ê≥ïÁöÑ„Ç¢„Éâ„Éê„Ç§„Çπ„Åß„ÅØ„Å™„ÅèÊÉÖÂ†±Êèê‰æõ„Åß„ÅÇ„Çã„Åì„Å®„ÇíÈÅ©Âàá„Å´‰ºù„Åà„Å¶„Åè„Å†„Åï„ÅÑ`;
  }

  private buildContextPrompt(context: SearchResult[]): string {
    if (context.length === 0) {
      return '„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà: Èñ¢ÈÄ£„Åô„ÇãÊñáÊõ∏„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ';
    }

    const contextText = context
      .map((result, index) => {
        const source = `„ÄêÊñáÊõ∏${index + 1}„Äë${result.chunk.title} (${result.chunk.documentPath})`;
        const content = result.chunk.content;
        const relevanceScore = `Èñ¢ÈÄ£Â∫¶: ${(result.score * 100).toFixed(1)}%`;
        
        return `${source}
${relevanceScore}

${content}

---`;
      })
      .join('\n\n');

    return `„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà (Èñ¢ÈÄ£„Åô„ÇãÊ≥ïÂæãÊñáÊõ∏):

${contextText}`;
  }

  private buildConversationHistory(conversation: ConversationMessage[]): string {
    if (conversation.length === 0) {
      return '';
    }

    const history = conversation
      .slice(-6) // Keep last 6 messages for context
      .map(msg => {
        const role = msg.role === 'user' ? '„É¶„Éº„Ç∂„Éº' : '„Ç¢„Ç∑„Çπ„Çø„É≥„Éà';
        return `${role}: ${msg.content}`;
      })
      .join('\n\n');

    return `‰ºöË©±Â±•Ê≠¥:
${history}

---`;
  }

  // Health check method for provider monitoring
  async healthCheck(): Promise<boolean> {
    try {
      const model = this.vertexAI.getGenerativeModel({
        model: this.model,
        generationConfig: {
          temperature: 0,
          maxOutputTokens: 10,
        },
      });

      const result = await model.generateContent('ÂÅ•Â∫∑„ÉÅ„Çß„ÉÉ„ÇØÁî®„ÅÆ„ÉÜ„Çπ„Éà„Åß„Åô„ÄÇ„ÄåOK„Äç„Å®ÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ');
      
      return !!(result.response && result.response.candidates?.[0]?.content?.parts?.[0]?.text);
    } catch (error) {
      console.error('Vertex AI LLM health check failed:', error);
      return false;
    }
  }

  // Get model information
  getModelInfo(): { name: string; maxTokens: number; supportsStreaming: boolean } {
    // Model specifications for common Vertex AI text models
    const modelSpecs: Record<string, { maxTokens: number; supportsStreaming: boolean }> = {
      'gemini-1.5-pro': { maxTokens: 2048, supportsStreaming: true },
      'gemini-1.5-flash': { maxTokens: 8192, supportsStreaming: true },
      'gemini-pro': { maxTokens: 2048, supportsStreaming: true },
    };

    const spec = modelSpecs[this.model] || { maxTokens: 2048, supportsStreaming: true };
    
    return {
      name: this.model,
      ...spec,
    };
  }

  // Estimate token usage for cost tracking
  estimateTokens(text: string): number {
    // Rough estimation: 1 token ‚âà 0.75 words for Japanese
    // This is a simplification; actual tokenization varies by model
    const words = text.split(/\s+/).length;
    const japaneseChars = (text.match(/[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9faf]/g) || []).length;
    
    // Estimate tokens considering both words and Japanese characters
    return Math.ceil(words * 0.75 + japaneseChars * 0.5);
  }
}

export const createVertexAILLMService = (config: VertexAIConfig): LLMProvider => {
  return new VertexAILLMService(config);
};